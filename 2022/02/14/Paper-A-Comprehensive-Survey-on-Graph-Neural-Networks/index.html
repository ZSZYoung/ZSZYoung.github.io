<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Roy Huang"><title>Paper: A Comprehensive Survey on Graph Neural Networks · Roy's Blog</title><meta name="description" content="作者與來源

Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang and P. S. Yu, &quot;A
Comprehensive Survey on Graph Neural Networks,&quot; in IEEE Transactions on
Neural N"><meta name="keywords" content="Blog, Computer Science"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- 加入测试--><!--link (rel="icon", href= url_for('images/favicon.ico')) --><script src="/js/jquery.js"></script><meta name="generator" content="Hexo 7.1.1"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title=""><a href="/">Roy's Blog</a></h3><div class="description"><p>何如暮暮與朝朝，更改卻，年年歲歲。</p></div></div></div><ul class="social-links"><li><a target="_blank" rel="noopener" href="https://github.com/ZSZYoung"><i class="fa fa-github"></i></a></li></ul><div class="footer"><div class="p"> <span>©  </span><i class="fa fa-star"></i><span> Roy Huang</span></div><div class="by_farbox"><span>Powered by </span><a href="https://hexo.io/zh-cn/" target="_blank">Hexo </a><span> & </span><a href="https://github.com/mrcore/hexo-theme-Anatole-Core" target="_blank">Anatole-Core  </a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/archives">Archives</a></li><li><a href="/tags">Tags</a></li><li><a href="/about">About</a></li><li><a href="/guestbook">Guestbook</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>Paper: A Comprehensive Survey on Graph Neural Networks</a></h3></div><div class="post-content"><h2 id="作者與來源">作者與來源</h2>
<blockquote>
<p>Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang and P. S. Yu, "A
Comprehensive Survey on Graph Neural Networks," in IEEE Transactions on
Neural Networks and Learning Systems, vol. 32, no. 1, pp. 4-24, Jan.
2021, doi: 10.1109/TNNLS.2020.2978386.</p>
</blockquote>
<hr>
<h2 id="四點貢獻">四點貢獻</h2>
<ul>
<li>New Taxonomy: GNNs are categorized into four groups: recurrent
GNNs(RecGNN), convolutional GNNs(ConvGNNs), graph autoencoders(GAEs),
and spatial-temporal GNNs(STGNNs)</li>
<li>Comprehensive Review: For each type of GNNs, we provide detailed
descriptions of representative models, make the necessary comparison and
summary the corresponding algorithms.</li>
<li>Abundant Resources: We collect abundant resources on GNNs, including
state-of-the-art models, benchmark data sets, open-source codes, and
practical applications.</li>
<li>Future Directions</li>
</ul>
<hr>
<h2 id="notation">Notation</h2>
<center>
<p><img src="https://cdn.jsdelivr.net/gh/ZSZYoung/Images-Host@main/blog/images/20220210171746.png"></p>
</center>
<hr>
<h2 id="gnn分類">GNN分類</h2>
<ul>
<li><p>Recurrent Graph Neural Networks: These are mostly pioneer works
of GNNs. RecGNNs aim to learn node representations with recurrent neural
architectures. They assume a node in a graph constantly exchanges
information/message with its neighbors until a stable equilibrium is
reached.</p></li>
<li><p>Convolutional Graph Neural Networks: These generalize the
operation of convolution from grid data to graph data. The main idea is
to generate a node <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.097ex" height="1.027ex" role="img" focusable="false" viewBox="0 -443 485 454"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g></g></svg></mjx-container></span>’s
representation by aggregating its own features <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.158ex" height="1.332ex" role="img" focusable="false" viewBox="0 -431 953.9 588.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z"></path></g></g><g data-mml-node="mi" transform="translate(561,-150) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g></g></g></svg></mjx-container></span> and neighbors’ features
<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.158ex" height="1.332ex" role="img" focusable="false" viewBox="0 -431 953.9 588.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z"></path></g></g><g data-mml-node="mi" transform="translate(561,-150) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g></g></g></svg></mjx-container></span>, where <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="8.927ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3945.6 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(849.8,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mi" transform="translate(1794.6,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mo" transform="translate(2682.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(3071.6,0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(3556.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span>. Different from RecGNNs,
ConvGNNs stack multiple graph convolutional layers to extract high-level
node representations.</p></li>
<li><p>Graph Autoencoders: These are unsupervised learning frameworks
that encode nodes/graphs into a latent vector space and reconstruct
graph data from the encoded information. GAEs are used to learn network
embeddings and graph generative distributions. For network embedding,
GAEs learn latent node representations through reconstructing graph
structural information, such as the graph adjacency matrix. For graph
generation, some methods generate nodes and edges of a graph step by
step, while other methods output a graph all at once.</p></li>
<li><p>Spatial–Temporal Graph Neural Networks: These aim to learn hidden
patterns from spatial–temporal graphs. The key idea of STGNNs is to
consider spatial dependence and temporal dependence at the same time.
Many current approaches integrate graph convolutions to capture spatial
dependence with RNNs or CNNs to model temporal dependence.</p></li>
</ul>
<center>
<p><img src="https://cdn.jsdelivr.net/gh/ZSZYoung/Images-Host@main/blog/images/20220210180207.png"></p>
</center>
<p>(a):ConvGNN with multiple graph convolutional layers</p>
<p>(b):ConvGNN with pooling and readout layers for graph
classification</p>
<p>(c):GAE for network embedding</p>
<p>(d):STGNN for spatial–temporal graph forecasting</p>
<hr>
<h2 id="三種典型應用">三種典型應用</h2>
<ul>
<li><p>Semisupervised Learning for Node-Level Classification: Given a
single network with partial nodes being labeled and others remaining
unlabeled, ConvGNNs can learn a robust model that effectively identifies
the class labels for the unlabeled nodes.
對於部分有標籤的node，可以用ConvGNN進行節點分類。</p></li>
<li><p>Supervised Learning for Graph-Level Classification:Graph-level
classification aims to predict the class label(s) for an entire
graph.</p></li>
<li><p>Unsupervised Learning for Graph Embedding: When no class labels
are available in graphs, we can learn the graph embedding in a purely
unsupervised way in an end-to-end framework.</p></li>
</ul>
<hr>
<h2 id="不同gnn模型比較">不同GNN模型比較</h2>
<center>
<p><img src="https://cdn.jsdelivr.net/gh/ZSZYoung/Images-Host@main/blog/images/20220210180432.png"></p>
</center>
<hr>
<h2 id="spatial-based-gcn-vs-spectral-based-gcn">Spatial-Based GCN VS
Spectral Based GCN</h2>
<ul>
<li><p>Spatial-Based: Analogous to the convolutional operation of a
conventional CNN on an image, spatial-based methods define graph
convolutions based on a node’s spatial relations. The spatial-based
graph convolutions convolve the central node’s representation with its
neighbors’ representations to derive the updated representation for the
central node.The spatial graph convolutional operation essentially
propagates node information along edges.</p></li>
<li><p>Spectral-Based: Spectral-based methods have a solid mathematical
foundation in graph signal processing, based on graph Laplacian. Due to
the eigen decomposition of the Laplacian matrix, spectral CNN faces
three limitations. <strong>（三種局限性）</strong> First, any
perturbation to a graph results in a change of eigenbasis. Second, the
learned filters are domain dependent, which means that they cannot be
applied to a graph with a different structure. Third, eigen
decomposition requires <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.832ex" height="2.451ex" role="img" focusable="false" viewBox="0 -833.2 2577.6 1083.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(1152,0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g></g><g data-mml-node="mo" transform="translate(2188.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span>
computational complexity.</p></li>
</ul>
<p>對比兩種方法：</p>
<blockquote>
<p>Spectral models have a theoretical foundation in graph signal
processing. By designing new graph signal filters , one can build new
ConvGNNs. However, spatial models are preferred over spectral models due
to efficiency, generality, and flexibility issues.
Spectral-based有更好的數學理論基礎保證，而spatial-based更具高效性、推廣性、靈活性。</p>
</blockquote>
<hr>
<h2 id="可用公開資料集">可用公開資料集</h2>
<center>
<p><img src="https://cdn.jsdelivr.net/gh/ZSZYoung/Images-Host@main/blog/images/20220210182210.png"></p>
</center>
<p>開源GNN庫:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://pytorch-geometric.readthedocs.io/en/latest/">Pytorch
Geometric</a></li>
<li><a target="_blank" rel="noopener" href="https://www.dgl.ai/">Deep Graph Library</a></li>
</ul>
<hr>
<h2 id="未來研究方向">未來研究方向</h2>
<ul>
<li><p>Model depth: As graph convolutions push representations of
adjacent nodes closer to each other, in theory, with an infinite number
of graph convolutional layers, all nodes’ representations will converge
to a single point. This raises the question of whether going deep is
still a good strategy for learning graph data.
一直加深模型的深度對graph類型的數據是否合適，值得探討。</p></li>
<li><p>Scalability Tradeoff:
在獲得GNN的scalability時，同時也犧牲了節點鄰居間的影響，結構方面的特征。所以如何均衡scalability和結構特征是一個可能的主題。</p></li>
<li><p>Heterogenity：現在研究的GNN都不適用於heterogenity
graph，即不適用於有不同類型node和edge的graph。</p></li>
<li><p>Dynamicity：現實中graph的node和edge往往是常常變化的，這影響了如何進行graph
convolution。</p></li>
</ul>
<hr>
<h2 id="參考">參考</h2>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.00596">A Comprehensive Survey on
Graph Neural Networks</a></li>
</ul>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2022-02-14</span><i class="fa fa-tag"></i><a class="tag" href="/tags/Machine-Learning/" title="Machine Learning">Machine Learning </a><a class="tag" href="/tags/GraphML/" title="GraphML">GraphML </a><span class="leancloud_visitors"></span></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="" onclick="javascript:join_favorite()" ref="sidebar"></a></div><div class="twitter"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=,https://r0y.tech/2022/02/14/Paper-A-Comprehensive-Survey-on-Graph-Neural-Networks/,Roy's Blog,Paper: A Comprehensive Survey on Graph Neural Networks,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2022/02/28/VSCode%E4%B8%8BMarkdown%E8%BD%89PDF/" title="VSCode下Markdown轉PDF">Prev</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2022/01/21/Jsch%E5%AF%A6%E8%B8%90/" title="Jsch實踐">Next</a></li></ul></div><script src="/js/visitors.js"></script><a id="comments"></a><div id="vcomments" style="margin:0 30px;"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/gh/xcss/valine@latest/dist/Valine.min.js"></script><script>var valine = new Valine({
  el:'#vcomments',
  notify:true || false, 
  verify:false|| false, 
  app_id:'LP5EBVNHfkmn17seVBnFrDIB-gzGzoHsz',
  app_key:'I1QAQGHsA6reNM2I3lwh3P8t',
  lang:'en',
  placeholder:'May the force be with you.',
  path: window.location.pathname,
  serverURLs: '',
  visitor:true,
  recordIP:true,
  avatar:'mp'
})</script></div></div></div></div><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/add-bookmark.js"></script><script src="/js/baidu-tongji.js"></script></body></html>